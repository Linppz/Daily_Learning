# ğŸ§  LLM Client å¼€å‘å­¦ä¹ ç¬”è®° â€” Task 2ï¼šæŠ½è±¡åŸºç±»

> ä½œè€…ï¼šYi  
> æ—¥æœŸï¼š2026-02-02  
> é˜¶æ®µï¼šTask 2 - å®šä¹‰æŠ½è±¡å±‚ (BaseLLM)

---

## ğŸ“š æœ¬èŠ‚æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. ä¸ºä»€ä¹ˆç”¨ `messages: List[Message]` è€Œä¸æ˜¯ `prompt: str`

ç°ä»£ LLM APIï¼ˆGPT-4ã€Claude ç­‰ï¼‰éƒ½åŸºäº **Chat Completion** æ¥å£ï¼Œéœ€è¦ä¼ é€’æ¶ˆæ¯åˆ—è¡¨ã€‚

**ä¸‰ä¸ªå…³é”®ä¼˜åŠ¿ï¼š**

| ç‰¹æ€§ | `List[Message]` | `str` |
|------|-----------------|-------|
| å¤šè½®å¯¹è¯ | âœ… å¤©ç„¶æ”¯æŒä¸Šä¸‹æ–‡è®°å¿† | âŒ éœ€è¦æ‰‹åŠ¨æ‹¼æ¥ |
| System Prompt | âœ… `role="system"` è®¾å®š AI äººè®¾ | âŒ æ— æ³•å®ç° |
| è§’è‰²åŒºåˆ† | âœ… æ˜ç¡®æ ‡è®° user/assistant | âŒ æ··ä¹±ä¸æ¸… |

**ç¤ºä¾‹ï¼š**
```python
messages = [
    Message(role="system", content="ä½ æ˜¯ä¸€ä¸ª Python ä¸“å®¶"),
    Message(role="user", content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
    Message(role="assistant", content="è£…é¥°å™¨æ˜¯..."),
    Message(role="user", content="ç»™æˆ‘ä¸€ä¸ªä¾‹å­")  # AI èƒ½ç†è§£è¿™æ˜¯åœ¨é—®è£…é¥°å™¨çš„ä¾‹å­
]
```

---

### 2. ä¸ºä»€ä¹ˆè¦ç‹¬ç«‹ä¼ é€’ `GenerationConfig`

**æ ¸å¿ƒæ€æƒ³ï¼šè§£è€¦ = è°ƒç”¨æ–¹å’Œé…ç½®æ–¹åˆ†ç¦»**

ä¸æ˜¯ä¸ºäº†"æ–¹ä¾¿"ï¼Œè€Œæ˜¯ä¸ºäº†**é¢„å®šä¹‰ + è¿è¡Œæ—¶åˆ‡æ¢**ï¼š

```python
# é¢„å®šä¹‰å¤šå¥—é…ç½®
creative_config = GenerationConfig(temperature=0.9, top_p=0.95)
coding_config = GenerationConfig(temperature=0.2, top_p=0.1)
strict_config = GenerationConfig(temperature=0.0, top_p=0.1)

# è¿è¡Œæ—¶æ ¹æ®åœºæ™¯åˆ‡æ¢ï¼Œä¸ç”¨æ”¹ä»»ä½•è°ƒç”¨ä»£ç 
await client.generate(messages, creative_config)   # å†™å°è¯´
await client.generate(messages, coding_config)     # å†™ä»£ç 
await client.generate(messages, strict_config)     # åšæ•°å­¦é¢˜
```

**å¥½å¤„ï¼š**
- æ”¹é…ç½®ä¸ç”¨æ”¹è°ƒç”¨ä»£ç 
- æ”¹è°ƒç”¨é€»è¾‘ä¸ç”¨ç®¡é…ç½®ç»†èŠ‚
- é…ç½®å¯å¤ç”¨ã€å¯ç‰ˆæœ¬ç®¡ç†

---

### 3. æŠ½è±¡åŸºç±» (ABC) çš„ä½œç”¨

**æŠ½è±¡åŸºç±» = å¥‘çº¦ = è§„èŒƒ**

```python
from abc import ABC, abstractmethod

class BaseLLM(ABC):
    @abstractmethod
    async def generate(self, messages, config) -> LLMResponse:
        pass

    @abstractmethod
    async def stream(self, messages, config) -> AsyncIterator[str]:
        pass
```

**ä¸ºä»€ä¹ˆç›´æ¥å®ä¾‹åŒ–ä¼šæŠ¥é”™æ˜¯"æˆåŠŸ"çš„æ ‡å¿—ï¼Ÿ**

```bash
>>> BaseLLM()
TypeError: Can't instantiate abstract class BaseLLM with abstract methods generate, stream
```

è¿™æ˜¯ **featureï¼Œä¸æ˜¯ bug**ï¼

**å¥½å¤„ï¼š**
1. **æœºå™¨å¼ºåˆ¶æ‰§è¡Œè§„èŒƒ** â€”â€” æ¯”æ–‡æ¡£é è°±ä¸€ä¸‡å€
2. **é˜²æ­¢é—æ¼å®ç°** â€”â€” å¦‚æœå­ç±»å¿˜äº†å®ç° `stream()`ï¼Œä»£ç ç›´æ¥è·‘ä¸èµ·æ¥
3. **å›¢é˜Ÿåä½œä¿éšœ** â€”â€” æ‰€æœ‰äººå¿…é¡»æŒ‰åŒä¸€ä¸ªæ¥å£å†™ä»£ç 

---

### 4. AsyncIterator ä¸æµå¼è¾“å‡º

`AsyncIterator[str]` ç”¨äºå®ç°"æ‰“å­—æœºæ•ˆæœ"ï¼š

```python
async def stream(self, messages, config) -> AsyncIterator[str]:
    full_text = "Hello World"
    for char in full_text:
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        yield char                 # é€å­—è¾“å‡º
```

**ä½¿ç”¨æ–¹å¼ï¼š**
```python
async for chunk in client.stream(messages, config):
    print(chunk, end="", flush=True)
```

---

## âš ï¸ æ˜“é”™ç‚¹æ€»ç»“

### é”™è¯¯ 1ï¼šå­ç±»ä½¿ç”¨ `@abstractmethod`

```python
# âŒ é”™è¯¯
class MockLLM(BaseLLM):
    @abstractmethod  # å­ç±»ä¸åº”è¯¥æœ‰è¿™ä¸ªï¼
    async def generate(self, ...):
        pass

# âœ… æ­£ç¡®
class MockLLM(BaseLLM):
    async def generate(self, ...):  # ç›´æ¥å®ç°ï¼Œä¸åŠ è£…é¥°å™¨
        return LLMResponse(...)
```

**è®°ä½ï¼š**
> åŸºç±»ç”¨ `@abstractmethod` **å£°æ˜**è§„èŒƒ  
> å­ç±»ç›´æ¥**å®ç°**æ–¹æ³•ä½“ï¼Œä¸åŠ ä»»ä½•è£…é¥°å™¨

---

### é”™è¯¯ 2ï¼šPydantic/Dataclass å®ä¾‹åŒ–æ–¹å¼

```python
# âŒ é”™è¯¯ - è¿™åªæ˜¯æ‹¿åˆ°ç±»æœ¬èº«
obj = LLMResponse
obj.content = "..."  # ä¿®æ”¹çš„æ˜¯ç±»å±æ€§ï¼

# âœ… æ­£ç¡® - è°ƒç”¨æ„é€ å‡½æ•°åˆ›å»ºå®ä¾‹
obj = LLMResponse(
    content="This is a mock response.",
    model="mock",
    usage={"prompt_tokens": 0, "completion_tokens": 0}
)
```

---

### é”™è¯¯ 3ï¼šå¿˜è®°å¯¼å…¥å¿…è¦æ¨¡å—

```python
# âŒ ç”¨äº† asyncio.sleep ä½†æ²¡å¯¼å…¥
await asyncio.sleep(0.1)  # NameError!

# âœ… è®°å¾—å¯¼å…¥
import asyncio
```

---

## ğŸ“ å®Œæ•´ä»£ç ç¤ºä¾‹ï¼šMockLLM

```python
import asyncio
from src.llm.base import BaseLLM
from src.llm.schemas import LLMResponse, Message, GenerationConfig
from typing import List, AsyncIterator


class MockLLM(BaseLLM):
    """ç”¨äºæµ‹è¯•çš„æ¨¡æ‹Ÿ LLM å®¢æˆ·ç«¯"""

    async def generate(
        self, 
        messages: List[Message], 
        config: GenerationConfig
    ) -> LLMResponse:
        """éæµå¼ï¼šç›´æ¥è¿”å›å›ºå®šå“åº”"""
        return LLMResponse(
            content="This is a mock response.",
            model="mock",
            usage={"prompt_tokens": 0, "completion_tokens": 0}
        )

    async def stream(
        self, 
        messages: List[Message], 
        config: GenerationConfig
    ) -> AsyncIterator[str]:
        """æµå¼ï¼šé€å­—è¾“å‡º"""
        full_text = "Hello World"
        for char in full_text:
            await asyncio.sleep(0.1)
            yield char
```

---

## ğŸ¯ æœ¬èŠ‚å…³é”®æ”¶è·

| æ¦‚å¿µ | ä¸€å¥è¯æ€»ç»“ |
|------|-----------|
| `List[Message]` | æ”¯æŒå¤šè½®å¯¹è¯ + System Prompt + è§’è‰²åŒºåˆ† |
| `GenerationConfig` | è§£è€¦ = é¢„å®šä¹‰é…ç½® + è¿è¡Œæ—¶åˆ‡æ¢ |
| æŠ½è±¡åŸºç±» (ABC) | æœºå™¨å¼ºåˆ¶æ‰§è¡Œçš„æ¥å£å¥‘çº¦ |
| `@abstractmethod` | åªåœ¨åŸºç±»ç”¨ï¼Œå­ç±»ç›´æ¥å®ç° |
| `AsyncIterator` | æµå¼è¾“å‡ºçš„æ ¸å¿ƒï¼Œé…åˆ `yield` ä½¿ç”¨ |

---

## ğŸš€ ä¸‹ä¸€æ­¥

Task 3ï¼šå®ç°å…·ä½“çš„ LLM Clientï¼ˆOpenAI / DeepSeekï¼‰

å°† `BaseLLM` è¿™ä¸ª"æ’åº§"æ¥ä¸ŠçœŸæ­£çš„"ç”µå™¨"ï¼
